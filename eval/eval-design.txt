这是一篇设计文档。
其目的是为今后的测评奠定框架。

* 性能指标
只采集一些基本数据，其他统计量在统计分析的时候再产生。
不要让ropevm去做复杂计算，它只进行收集。
每个线程分别收集数据,将来既可以画合计的图,也可以分别作图.
就认为一条指令之用一个时钟周期。


空闲周期数 idle-cycle-count
繁忙周期数 busy-cycle-count

验证成功数 verify-ok-count
验证错误数 verify-mismatch-count
无可验证数 verify-empty-count
(后两者合计为验证失败数verify-failed-count)

确定模式周期数 cert-mode-cycle-count
推测模式周期数 spec-mode-cycle-count
rvp模式周期数 rvp-mode-cycle-count

** 其他指标
（这些指标我还没有想好怎么去用）


确定控制转移多少次？这个应当等于确定消息发送的数目。

在推测模式下等待多少周期？

发送推测消息多少条？
处理推测消息多少条？
召回推测消息多少条？

同步消息多少条?
异步消息多少条?

共产生多少个对象
共用了多少个线程
平均一个线程负责多少个对象

停机多少次，原因都是什么？每种原因所占比例。

消息队列的长度：最大长度，平均长度。
其中待处理部分的长度，待提交部分的长度。
多版本状态缓冲区中版本数(待提交状态快照的数目)

* 实施
建立专门用于测评的eval目录。
并将其至于版本控制之下，以后的测评在以前的基础上进行修改。
每投一篇文章，都要对源码树进行快照。


eval目录下的java程序与开发时所用的java程序之间是什么关系？

- eval下的java程序应该是已经稳定下来的程序。这个是必然的。
- eval下的java程序必须是出现在论文中的。其他东西会妨碍测评。

将被测程序复制到eval文件夹下。
文件夹名改为treeadd-1-2这种形式。
第一个数字是程序版本号，第二个数字是输入版本号。


在overview.txt中说明都对哪些程序进行了测试.不同的配置都是什么.

最好能这样：
在eval文件夹下，我运行一个名为start-eval的脚本。
该脚本自动地去运行各个目录下的程序，产生统计输出。
统计输出文件的格式应方便导入excel，导入后对应一张worksheet。

** 发表分支
以用为目的的程序有发布分支，以发表为目的的程序就该有发表分支。
这对于研究团队来说很有意义，不过现在先不整。

** 发表快照
发表分支的一个代替品是发表快照

* 输出文件的格式
其格式应方便导入excel
第一列 线程id
第二列 统计量
第三列 数值

首先要保证涵盖那四个图
加速比
繁忙和空闲比
验证成功率
各种模式所占时间比例


* ropevm对统计的支持
用命令行开关或环境变量控制是否进行统计。（默认为关闭）
宏的支持。用于统计的代码。大段代码。
统计数据收集在什么地方。
